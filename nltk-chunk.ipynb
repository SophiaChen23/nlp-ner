{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import operator\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import simplejson as json\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "import logging\n",
    "import csv\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the petitions data(can use the dataset to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs=[]\n",
    "with open('petitions_complete.csv') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            docs.append(row)\n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "for passage in docs:\n",
    "    passage['body']=[x for x in word_tokenize(passage['body']) if x.isalpha()]\n",
    "    \n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  make feature function for NamedEntityChunker model, which is necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "corpus_root = \"gmb-2.2.0\" \n",
    " \n",
    "def features(tokens, index, history):\n",
    "    \"\"\"\n",
    "    `tokens`  = a POS-tagged sentence [(w1, t1), ...]\n",
    "    `index`   = the index of the token we want to extract features for\n",
    "    `history` = the previous predicted IOB tags\n",
    "    \"\"\"\n",
    " \n",
    "    # init the stemmer\n",
    "    stemmer = SnowballStemmer('english')\n",
    " \n",
    "    # Pad the sequence with placeholders\n",
    "    tokens = [('[START2]', '[START2]'), ('[START1]', '[START1]')] + list(tokens) + [('[END1]', '[END1]'), ('[END2]', '[END2]')]\n",
    "    history = ['[START2]', '[START1]'] + list(history)\n",
    " \n",
    "    # shift the index with 2, to accommodate the padding\n",
    "    index += 2\n",
    " \n",
    "    word, pos = tokens[index]\n",
    "    prevword, prevpos = tokens[index - 1]\n",
    "    prevprevword, prevprevpos = tokens[index - 2]\n",
    "    nextword, nextpos = tokens[index + 1]\n",
    "    nextnextword, nextnextpos = tokens[index + 2]\n",
    "    previob = history[index - 1]\n",
    "    contains_dash = '-' in word\n",
    "    contains_dot = '.' in word\n",
    "    allascii = all([True for c in word if c in string.ascii_lowercase])\n",
    " \n",
    "    allcaps = word == word.capitalize()\n",
    "    capitalized = word[0] in string.ascii_uppercase\n",
    " \n",
    "    prevallcaps = prevword == prevword.capitalize()\n",
    "    prevcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    nextallcaps = prevword == prevword.capitalize()\n",
    "    nextcapitalized = prevword[0] in string.ascii_uppercase\n",
    " \n",
    "    return {\n",
    "        'word': word,\n",
    "        'lemma': stemmer.stem(word),\n",
    "        'pos': pos,\n",
    "        'all-ascii': allascii,\n",
    " \n",
    "        'next-word': nextword,\n",
    "        'next-lemma': stemmer.stem(nextword),\n",
    "        'next-pos': nextpos,\n",
    " \n",
    "        'next-next-word': nextnextword,\n",
    "        'nextnextpos': nextnextpos,\n",
    " \n",
    "        'prev-word': prevword,\n",
    "        'prev-lemma': stemmer.stem(prevword),\n",
    "        'prev-pos': prevpos,\n",
    " \n",
    "        'prev-prev-word': prevprevword,\n",
    "        'prev-prev-pos': prevprevpos,\n",
    " \n",
    "        'prev-iob': previob,\n",
    " \n",
    "        'contains-dash': contains_dash,\n",
    "        'contains-dot': contains_dot,\n",
    " \n",
    "        'all-caps': allcaps,\n",
    "        'capitalized': capitalized,\n",
    " \n",
    "        'prev-all-caps': prevallcaps,\n",
    "        'prev-capitalized': prevcapitalized,\n",
    " \n",
    "        'next-all-caps': nextallcaps,\n",
    "        'next-capitalized': nextcapitalized,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Groningen Meaning Bank (GMB) though.\n",
    "# GMB is a fairly large corpus with a lot of annotations\n",
    "# import corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conll_iob(annotated_sentence):\n",
    "    \"\"\"\n",
    "    `annotated_sentence` = list of triplets [(w1, t1, iob1), ...]\n",
    "    Transform a pseudo-IOB notation: O, PERSON, PERSON, O, O, LOCATION, O\n",
    "    to proper IOB notation: O, B-PERSON, I-PERSON, O, O, B-LOCATION, O\n",
    "    \"\"\"\n",
    "    proper_iob_tokens = []\n",
    "    for idx, annotated_token in enumerate(annotated_sentence):\n",
    "        tag, word, ner = annotated_token\n",
    " \n",
    "        if ner != 'O':\n",
    "            if idx == 0:\n",
    "                ner = \"B-\" + ner\n",
    "            elif annotated_sentence[idx - 1][2] == ner:\n",
    "                ner = \"I-\" + ner\n",
    "            else:\n",
    "                ner = \"B-\" + ner\n",
    "        proper_iob_tokens.append((tag, word, ner))\n",
    "    return proper_iob_tokens\n",
    " \n",
    " \n",
    "def read_gmb(corpus_root):\n",
    "    for root, dirs, files in os.walk(corpus_root):\n",
    "        for filename in files:\n",
    "            if filename.endswith(\".tags\"):\n",
    "                with open(os.path.join(root, filename), 'rb') as file_handle:\n",
    "                    file_content = file_handle.read().decode('utf-8').strip()\n",
    "                    annotated_sentences = file_content.split('\\n\\n')\n",
    "                    for annotated_sentence in annotated_sentences:\n",
    "                        annotated_tokens = [seq for seq in annotated_sentence.split('\\n') if seq]\n",
    " \n",
    "                        standard_form_tokens = []\n",
    " \n",
    "                        for idx, annotated_token in enumerate(annotated_tokens):\n",
    "                            annotations = annotated_token.split('\\t')\n",
    "                            word, tag, ner = annotations[0], annotations[1], annotations[3]\n",
    " \n",
    "                            if ner != 'O':\n",
    "                                ner = ner.split('-')[0]\n",
    " \n",
    "                            if tag in ('LQU', 'RQU'):   # Make it NLTK compatible\n",
    "                                tag = \"``\"\n",
    " \n",
    "                            standard_form_tokens.append((word, tag, ner))\n",
    " \n",
    "                        conll_tokens = to_conll_iob(standard_form_tokens)\n",
    " \n",
    "                        # Make it NLTK Classifier compatible - [(w1, t1, iob1), ...] to [((w1, t1), iob1), ...]\n",
    "                        # Because the classfier expects a tuple as input, first item input, second the class\n",
    "                        yield [((w, t), iob) for w, t, iob in conll_tokens]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import Iterable\n",
    "from nltk.tag import ClassifierBasedTagger\n",
    "from nltk.chunk import ChunkParserI\n",
    " \n",
    " \n",
    "class NamedEntityChunker(ChunkParserI):\n",
    "    def __init__(self, train_sents, **kwargs):\n",
    "        assert isinstance(train_sents, Iterable)\n",
    " \n",
    "        self.feature_detector = features\n",
    "        self.tagger = ClassifierBasedTagger(\n",
    "            train=train_sents,\n",
    "            feature_detector=features,\n",
    "            **kwargs)\n",
    " \n",
    "    def parse(self, tagged_sent):\n",
    "        chunks = self.tagger.tag(tagged_sent)\n",
    " \n",
    "        # Transform the result from [((w1, t1), iob1), ...] \n",
    "        # to the preferred list of triplets format [(w1, t1, iob1), ...]\n",
    "        iob_triplets = [(w, t, c) for ((w, t), c) in chunks]\n",
    "        iob_triplets=[(w, t, c) for (w, t, c) in iob_triplets if c !='O']\n",
    "        # Transform the list of triplets to nltk.Tree format\n",
    "        return conlltags2tree(iob_triplets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62010\n",
      "#training samples = 55809\n",
      "#test samples = 6201\n"
     ]
    }
   ],
   "source": [
    "reader = read_gmb(corpus_root)\n",
    "data = list(reader)\n",
    "training_samples = data[:int(len(data) * 0.9)]\n",
    "test_samples = data[int(len(data) * 0.9):]\n",
    "print(len(data))\n",
    "print (\"#training samples = %s\" % len(training_samples))    # training samples = 55809\n",
    "print (\"#test samples = %s\" % len(test_samples))                # test samples = 6201\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(('Thousands', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('Kenyans', 'NNS'), 'B-gpe'),\n",
       "  (('demonstrated', 'VBD'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('Nairobi', 'NNP'), 'B-geo'),\n",
       "  (('Tuesday', 'NNP'), 'B-tim'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('protest', 'VB'), 'O'),\n",
       "  (('last', 'JJ'), 'O'),\n",
       "  (('week', 'NN'), 'O'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('police', 'NN'), 'O'),\n",
       "  (('raids', 'NNS'), 'O'),\n",
       "  (('on', 'IN'), 'O'),\n",
       "  (('a', 'DT'), 'O'),\n",
       "  (('national', 'JJ'), 'O'),\n",
       "  (('media', 'NNS'), 'O'),\n",
       "  (('company', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('At', 'IN'), 'O'),\n",
       "  (('least', 'JJS'), 'O'),\n",
       "  (('2000', 'CD'), 'B-tim'),\n",
       "  (('people', 'NNS'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('including', 'VBG'), 'O'),\n",
       "  (('opposition', 'NN'), 'O'),\n",
       "  (('lawmakers', 'NNS'), 'O'),\n",
       "  (('from', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Orange', 'NNP'), 'B-org'),\n",
       "  (('Democratic', 'NNP'), 'I-org'),\n",
       "  (('Movement', 'NNP'), 'I-org'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('took', 'VBD'), 'O'),\n",
       "  (('part', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('demonstrations', 'NNS'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Several', 'JJ'), 'O'),\n",
       "  (('called', 'VBN'), 'O'),\n",
       "  (('for', 'IN'), 'O'),\n",
       "  (('high-ranking', 'JJ'), 'O'),\n",
       "  (('officials', 'NNS'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('resign', 'VB'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Kenyan', 'JJ'), 'B-gpe'),\n",
       "  (('police', 'NNS'), 'O'),\n",
       "  (('say', 'VBP'), 'O'),\n",
       "  (('Thursday', 'NNP'), 'B-tim'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('raids', 'NNS'), 'O'),\n",
       "  (('on', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Standard', 'JJ'), 'O'),\n",
       "  (('newspaper', 'NN'), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Kenya', 'NNP'), 'B-org'),\n",
       "  (('Television', 'NNP'), 'I-org'),\n",
       "  (('Network', 'NNP'), 'I-org'),\n",
       "  (('were', 'VBD'), 'O'),\n",
       "  (('a', 'DT'), 'O'),\n",
       "  (('matter', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('national', 'JJ'), 'O'),\n",
       "  (('security', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('But', 'CC'), 'O'),\n",
       "  (('several', 'JJ'), 'O'),\n",
       "  (('members', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('President', 'NNP'), 'B-per'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('Mwai', 'NNP'), 'B-per'),\n",
       "  (('Kibaki', 'NNP'), 'I-per'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('cabinet', 'NN'), 'O'),\n",
       "  (('condemned', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('raids', 'NNS'), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('country', 'NN'), 'O'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('press', 'NN'), 'O'),\n",
       "  (('called', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('actions', 'NNS'), 'O'),\n",
       "  (('shameful', 'JJ'), 'O'),\n",
       "  (('thuggery', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Witnesses', 'NNS'), 'O'),\n",
       "  (('say', 'VBP'), 'O'),\n",
       "  (('police', 'NN'), 'O'),\n",
       "  (('confiscated', 'VBD'), 'O'),\n",
       "  (('equipment', 'NN'), 'O'),\n",
       "  (('and', 'CC'), 'O'),\n",
       "  (('burned', 'VBD'), 'O'),\n",
       "  (('thousands', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('copies', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('newspapers', 'NNS'), 'O'),\n",
       "  (('during', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('raids', 'NNS'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('The', 'DT'), 'O'),\n",
       "  (('National', 'NNP'), 'O'),\n",
       "  (('Security', 'NNP'), 'O'),\n",
       "  (('Minister', 'NNP'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('John', 'NNP'), 'B-per'),\n",
       "  (('Michuki', 'NNP'), 'I-per'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('defended', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('raids', 'NNS'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('saying', 'VBG'), 'O'),\n",
       "  (('\"', '``'), 'O'),\n",
       "  (('if', 'IN'), 'O'),\n",
       "  (('you', 'PRP'), 'O'),\n",
       "  (('rattle', 'VBP'), 'O'),\n",
       "  (('a', 'DT'), 'O'),\n",
       "  (('snake', 'NN'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('you', 'PRP'), 'O'),\n",
       "  (('must', 'MD'), 'O'),\n",
       "  (('be', 'VB'), 'O'),\n",
       "  (('prepared', 'VBN'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('be', 'VB'), 'O'),\n",
       "  (('bitten', 'VBN'), 'O'),\n",
       "  (('by', 'IN'), 'O'),\n",
       "  (('it', 'PRP'), 'O'),\n",
       "  (('\"', '``'), 'O'),\n",
       "  (('alluding', 'VBG'), 'O'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Standard', 'NNP'), 'B-org'),\n",
       "  ((\"'s\", 'POS'), 'O'),\n",
       "  (('often', 'RB'), 'O'),\n",
       "  (('critical', 'JJ'), 'O'),\n",
       "  (('coverage', 'NN'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('government', 'NN'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Both', 'DT'), 'O'),\n",
       "  (('outlets', 'NNS'), 'O'),\n",
       "  (('have', 'VBP'), 'O'),\n",
       "  (('now', 'RB'), 'O'),\n",
       "  (('resumed', 'VBN'), 'O'),\n",
       "  (('their', 'PRP$'), 'O'),\n",
       "  (('operations', 'NNS'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('Several', 'JJ'), 'O'),\n",
       "  (('dozen', 'NN'), 'O'),\n",
       "  (('Palestinian', 'JJ'), 'B-gpe'),\n",
       "  (('police', 'NN'), 'O'),\n",
       "  (('stormed', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('Palestinian', 'JJ'), 'B-gpe'),\n",
       "  (('parliament', 'NN'), 'O'),\n",
       "  (('building', 'NN'), 'O'),\n",
       "  (('Monday', 'NNP'), 'B-tim'),\n",
       "  (('to', 'TO'), 'O'),\n",
       "  (('press', 'VB'), 'O'),\n",
       "  (('demands', 'NNS'), 'O'),\n",
       "  (('for', 'IN'), 'O'),\n",
       "  (('a', 'DT'), 'O'),\n",
       "  (('security', 'NN'), 'O'),\n",
       "  (('crackdown', 'NN'), 'O'),\n",
       "  (('on', 'IN'), 'O'),\n",
       "  (('Hamas', 'NNP'), 'B-org'),\n",
       "  (('militants', 'NNS'), 'O'),\n",
       "  (('.', '.'), 'O')],\n",
       " [(('There', 'EX'), 'O'),\n",
       "  (('were', 'VBD'), 'O'),\n",
       "  (('no', 'DT'), 'O'),\n",
       "  (('reports', 'NNS'), 'O'),\n",
       "  (('of', 'IN'), 'O'),\n",
       "  (('shooting', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('building', 'NN'), 'O'),\n",
       "  ((',', ','), 'O'),\n",
       "  (('but', 'CC'), 'O'),\n",
       "  (('witnesses', 'NNS'), 'O'),\n",
       "  (('say', 'VBP'), 'O'),\n",
       "  (('gunshots', 'NNS'), 'O'),\n",
       "  (('were', 'VBD'), 'O'),\n",
       "  (('fired', 'VBN'), 'O'),\n",
       "  (('as', 'IN'), 'O'),\n",
       "  (('police', 'NN'), 'O'),\n",
       "  (('entered', 'VBD'), 'O'),\n",
       "  (('the', 'DT'), 'O'),\n",
       "  (('legislative', 'JJ'), 'O'),\n",
       "  (('compound', 'NN'), 'O'),\n",
       "  (('in', 'IN'), 'O'),\n",
       "  (('Gaza', 'NNP'), 'B-geo'),\n",
       "  (('City', 'NNP'), 'I-geo'),\n",
       "  (('.', '.'), 'O')]]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = NamedEntityChunker(training_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre function to formate tokenize data and remove stop word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "\n",
    "def pre(passage):\n",
    "\n",
    "    passage=[x for x in word_tokenize(passage) if x.isalpha()]\n",
    "    ps = PorterStemmer()\n",
    "    passage1=[]\n",
    "    for word in passage:\n",
    "        passage1.append(ps.stem(word))\n",
    "    print(passage1)\n",
    "    passage1=list(set(passage1))\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    wordsFiltered = []\n",
    "    for w in passage1:\n",
    "        if w not in stopWords:\n",
    "            wordsFiltered.append(w)\n",
    "    return(wordsFiltered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"Cohen, who has been under criminal investigation for months, which is separate from the special counsel case, has been rushing to meet U.S. District Court for the Southern District of New York Judge Kimba Wood’s Friday deadline to complete a privilege review of over 3.7 million documents seized in the April 9 raids of Cohen’s New York properties and law office.Representatives from the Southern District of New York declined to comment.Cohen, who is under federal investigation now with no legal representation, is likely to cooperate with federal prosecutors in Manhattan, sources said. This development, which is believed to be imminent, will likely hit the White House, family members, staffers and counsels hard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get train rescult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cohen', 'who', 'ha', 'been', 'under', 'crimin', 'investig', 'for', 'month', 'which', 'is', 'separ', 'from', 'the', 'special', 'counsel', 'case', 'ha', 'been', 'rush', 'to', 'meet', 'district', 'court', 'for', 'the', 'southern', 'district', 'of', 'new', 'york', 'judg', 'kimba', 'wood', 's', 'friday', 'deadlin', 'to', 'complet', 'a', 'privileg', 'review', 'of', 'over', 'million', 'document', 'seiz', 'in', 'the', 'april', 'raid', 'of', 'cohen', 's', 'new', 'york', 'properti', 'and', 'law', 'from', 'the', 'southern', 'district', 'of', 'new', 'york', 'declin', 'to', 'who', 'is', 'under', 'feder', 'investig', 'now', 'with', 'no', 'legal', 'represent', 'is', 'like', 'to', 'cooper', 'with', 'feder', 'prosecutor', 'in', 'manhattan', 'sourc', 'said', 'thi', 'develop', 'which', 'is', 'believ', 'to', 'be', 'immin', 'will', 'like', 'hit', 'the', 'white', 'hous', 'famili', 'member', 'staffer', 'and', 'counsel', 'hard']\n",
      "(S\n",
      "  (org white/JJ)\n",
      "  (art properti/NN)\n",
      "  (tim separ/NN cooper/NN ha/NN feder/NN case/NN deadlin/NN)\n",
      "  (tim\n",
      "    april/JJ\n",
      "    represent/NN\n",
      "    hous/JJ\n",
      "    york/NN\n",
      "    million/CD\n",
      "    seiz/JJ\n",
      "    manhattan/NNS\n",
      "    judg/NN\n",
      "    court/NN)\n",
      "  (tim cohen/JJ)\n",
      "  (tim friday/JJ))\n"
     ]
    }
   ],
   "source": [
    "print(chunker.parse(pos_tag(pre(data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the petition dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for passage in docs:\n",
    "    passage['body']=chunker.parse((pos_tag(passage['body'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (org Keystone/NNP XL/NNP)\n",
      "  (tim sands/NNS in/IN)\n",
      "  (geo Alberta/NNP)\n",
      "  (org Big/NNP Oil/NN)\n",
      "  (gpe Canadian/NNP)\n",
      "  (nat Additionally/NNP tar/NN)\n",
      "  (org tar/NN)\n",
      "  (org Keystone/NNP XL/NNP permit/NN))\n"
     ]
    }
   ],
   "source": [
    "print((docs[1][\"body\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.49791790600832836\n"
     ]
    }
   ],
   "source": [
    "score = chunker.evaluate([conlltags2tree([(w, t, iob) for (w, t), iob in iobs if iob !='O']) for iobs in test_samples[:500]])\n",
    "print (score.accuracy()  )      # 0.931132334092 - Awesome :D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only test the accuracy of the dataset iob !=\"O\"( they are the time and location feature),\n",
    "# the accuracy is not great as the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
